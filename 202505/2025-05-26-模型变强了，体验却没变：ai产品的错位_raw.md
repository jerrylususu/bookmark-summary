Title: 模型变强了，体验却没变：AI产品的错位

URL Source: https://grapeot.me/ai-products.html

Markdown Content:
我一直是AI的坚定支持者。身边不少朋友问起AI能干什么，我会给出非常具体的答案：它可以帮你规划日程、调研资料，甚至在生活里处理很多琐碎但耗时的任务比如下单和砍价。我会根据他们的场景给出prompt模板，尽量降低他们尝试的门槛。但我这一两年向几百个人推荐了AI，观察是大多数人用完之后的反馈相当冷淡。最常见的回应是：还好吧，没觉得多有用。有时候还会补一刀：还不如我自己上手做嘞。

这个反差倒没有浇灭我给AI带路的热情，但确实一直让我困惑：明明模型变得越来越强，为什么普通用户却一直很难感受到？除了“AI是一个需要学习的工具”以外，还有没有什么更深层的原因？最近一段时间，我更系统地对比了市面上几个主流AI客户端使用体验（Claude、Gemini和ChatGPT，也有Deepseek），才慢慢意识到，这种AI好不好用的感受错位，是因为在用户和模型之外，连接二者的产品设计出了问题。

AI模型的能力确实在以惊人的速度演进。我们下面会介绍，它开始只是multi-turn conversation，后来加入了multi-modal的能力。现在最先进的AI，可以和各种工具交互，实现multi-hour的自主工作。但现在的app，大多数还停留在multi-turn的设计理念，跟LLM的能力差异巨大。所以当AI的智能通过一个不合适的交互介质呈现出来的时候，用户就会很抓狂。比如Claude app是为了短时间对话设计的，一旦切到后台任务执行就断了。那Claude 4再牛逼，能在后台执行几个小时的任务也没用。这就像把一台F1引擎塞进了桑塔纳里，牛逼吗？牛逼。好用吗？开起来跟普桑也差不多。

遗憾的是，这些app使用的细节，构成了用户感知的全部。绝大多数用户不知道这是app的问题，只会觉得AI不好用。这构成了AI产品的一个系统性的错位，也是这篇文章想详细讨论的话题。

Multi-Turn, Multi-Modal, 到 Multi-Hour Agency
--------------------------------------------

近两年AI模型的能力，出现了三次跃变。先是学会了记住上下文，能多轮对话；接着是可以看图、听声音、分析视频；现在最新的模型，甚至可以自主运行好几个小时，完成复杂任务、自动调用工具、阶段性地总结和反馈。

这三次跃变，从“能说”到“能看能听”，再到“能做”，一步步把AI从一个问答工具推向了智能助手。OpenAI在朝这个方向走，Google在走，Anthropic也在。但问题是，我们今天大多数用到的AI App，还是停留在两年前的那一代交互逻辑上。像是在一台桑塔纳，发动机已经逐渐升级成了f1的发动机，但刹车和悬挂全都没变。这才是很多人感受不到AI有多厉害的根本原因：模型在进化，但App没跟上。

### Multi-Turn：Chatbot的开端

多轮对话，是今天所有主流模型最基础的能力。

ChatGPT之所以成功，一个重要原因就是它不是像Google智能搜索那种问一句答一句的搜索框，而是能围绕一个任务持续对话的系统。这背后的关键技术是 Supervised Fine-Tuning（SFT），也就是用人类标注的多轮对话数据，去让模型学会怎么提取记忆，回答问题。

Claude在这方面也表现不错。它很擅长对上下文进行归纳和引用，比如帮你读论文、总结长文档，或者多轮润色一篇文章，做得很棒。

在这个阶段，做app很简单，基本上只要维护好聊天历史，在API外面套个壳就差不多了。各家的体验也都差不多。唯一要注意的就是对大context window的支持。比如Gemini 2.5系列模型支持1M级别的context window，这对于很多应用是非常重要的。但是它的网页端和客户端都会在用户输入了几千个token(也就是占用了不到1%模型能力)的时候假死，导致几乎不可用。这是一个app没跟上的典型例子。

把产品做成Chatbot的这种设计，放在2023年没关系，大家刚开始用AI就是为了聊天。但现在模型已经不是单纯的聊天机器了，而是一个可以处理结构化任务的copilot系统。如果App还停留在老思路里，就会极大地浪费模型的潜力。

### Multi-Modal：从能说到能看能听

第二次跃变是多模态。

今天主流模型都声称支持多模态，但差异很大。Gemini 2.5目前是这方面做得最彻底的。它可以原生看图、听音频、理解视频。而且不是简单地看，而是可以真正推理、组合、分析、总结。它背后的技术路线是，用不同的tokenizer结合projection layer把不同模态的信息（图像、声音、文本）映射到一个共享的表示空间里，让模型可以像看文字一样处理视频里的动作、语音里的语气。

OpenAI的路线类似，但没有一个统一的模型可以又实现推理（类似o3），又可以处理视频，音频和图像（类似gpt-4o-realtime）。它的亮点是，允许图像作为工具调用的对象。比如o3可以通过撰写 Python 代码对图像进行裁切、放大、识别图中细节，再把处理结果传回模型继续tokenize进一步分析。这种方式目前极大提升了它的多模态能力，甚至支撑了“看图猜地点”这种只有o3才能做到的变态场景。

Claude目前对多模态的支持比较基础，只能进行图像识别，不能处理音频或视频。

但是从体验来看，最先进的Gemini，反而是体验最差的一个。因为它的网页端和客户端根本不支持上传视频和音频，而只能上传图片。这是一个典型的模型活在2026，产品还在2023的例子。产品没有适配模型的竞争力，用户体验自然也很难做出差异化。

### Multi-Hour Agency：AI真正成了助理

第三个变化，是AI模型开始有了持续运行，自主完成任务的能力。我们可以把这个阶段叫做 Multi-Hour Agency，也就是 AI 能够维持上下文、调度工具，连续完成一个耗时几十分钟甚至几小时的任务，而不需要你每次去踢一脚动一下。

这其实是AI变得真正可用的前提。很多重要的事，比如调研某个领域的新闻、规划一个完整的旅程、分析一个数据库、生成一段结构清晰的代码，这些都超出了问答机器人的范畴。它们本质上需要的是一个能思考、能调用工具补充信息、能一步步自动执行甚至动态调整计划的系统。

Claude 4就声称自己可以连续跑七小时来完成一个特别复杂的任务。o3也能调用很多工具，分阶段执行非常复杂的任务。这些能力的实现，背后其实是对HFRL（人类反馈强化学习）、函数调用、外部工具接入、长上下文等机制的不断调优。模型本身已经准备好了接管一段复杂流程，但App没准备好。比如Claude模型再牛逼，iOS App甚至Mac App只要熄屏就断掉，聊天记录都找不回。

从多轮对话，到多模态理解，再到长时间任务执行，模型的能力一层层叠加。而App的能力几乎原地踏步。模型已经不是那个我问你答的机器人，而是一个可以和你共同完成任务的数字助理。但客户端在产品设计层面还把它仅仅当成一个延迟更低、语气更自然的搜索引擎。所以问题不在AI是不是够聪明，而是我们有没有构建出一个足够能承接这份聪明的产品结构。绝大多数时候，用户并不是在评价模型，而是在评价模型以某种形式被封装后的那层外壳。而那一层，很多公司（包括大公司）根本没花心思去做。

OpenAI，Claude，Gemini三大平台产品对比
----------------------------

说到底，AI模型的能力现在已经高度趋同，都是大模型+工具系统+长上下文+多模态编码。但真正拉开差距的，不是模型能力，而是产品怎么把这些能力跟用户的应用场景结合起来。我过去几个月持续在用 Claude、ChatGPT 和 Gemini，不光用了API，也用了GUI/app，不光用了Web端，也用了它们的iOS App和桌面端。整体感受是：三家公司都在强调自己有多强，但他们的消费级产品（除了OpenAI）用起来都像是在半成品和试验品之间切换。这一章我们就从用户的角度，看看三家在客户端上的优劣。

### Claude：模型扎实，App是个半成品

Claude 3.7/4这系列模型本身是很强的，尤其在长文本阅读、写代码、不偷懒这些方面，甚至比o3还要稳，Cursor上收获了无数好评，是很多人的go to model。但Claude.ai这个消费级产品的体验真是一言难尽。

Claude的客户端有一个非常致命的问题：你只要切App，推理就断了。不是说任务暂停或者重新连接，而是整个对话从历史里面直接消失。它不会告诉你中断了，但是任务状态直接变成空白，聊天在历史里也变成Untitled。不论是在iOS上熄屏，还是在Mac上把笔记本合上，都会触发这个问题。这个问题从根本上看，是因为Claude的消费级产品还没有从chatbot的思路里跳出来，觉得app就是API的一个wrapper而已。所以它的架构高度依赖于客户端，把stream的维持、session状态的保存，全部放在用户侧。这在只跑一个短问答的时候没问题，但一旦跑复杂任务，就完全撑不住。它的iOS app实现也很初级，模型的输出一长，手机就发烫。所以模型再强，用户只会说一句话：不好用。

这里面唯一的差异化因素，可能是Claude桌面app是目前唯一集成了MCP的主流客户端。可以直接利用MCP把本地资源接入消费级的AI平台，用订阅而不是token计费，这一点还是蛮实用的。

### Gemini：模型很强，App体验像个demo

Google的Gemini是一个更极端的例子：模型能力离谱地强，App做得离谱地差。

AI Studio是Google面向开发者的一个debug套件。在这个工具里，Gemini是我目前看到支持最大token window、最稳健的视频+音频+图片+文本混合分析的模型。上传100万字文档没压力，跑个10分钟的论文总结也不掉线。你给它100个重复任务，要它做一些枯燥的重复处理，Gemini也能不偷懒，不折不扣地完成。它的multi modal，tool use，尤其是instruction following的能力是业界顶级的，我个人甚至认为它把第二梯队的模型，包括Claude和GPT都甩开了一大截。

问题是，这一切都只能在Web版的AI Studio里体验到。这毕竟是个面向开发者的工具。全程要盯着网页前台，手机锁屏就掉线，system prompt每轮都会自动清空，没办法个性化，聊天记录的保存和分享完全依托google drive，也很初级。

面向消费级用户，google主推的是Gemini App。但这个App，就。。。尼玛非常离谱的一个产品，感觉是产品部门专门做出来恶心AI部门的。你Gemini 2.5模型不是1M context window吗？好的，我让用户输入10k左右token的提示词就把UI卡死，把你拉到跟其他AI同一个起跑线上。你Gemini 2.5不是处理视频和音频特别牛逼，别家都没有这个功能吗？好的，我在UI上就不允许用户上传视频和音频文件，这跟其他AI产品功能不就一样了吗。2025年年中才允许用户设置Gemini 2.5的系统提示（BTW现在网页版还有bug，移动版还没上线）。就算我终于找到一个场景可以用Gemini App了，也会发现它体现的智能和AI Studio里面的智能差距还是很大，会更厌恶用搜索来增加答案的广度，更倾向章口就莱，也不知道system prompt里面做了什么负优化。

所以很多人，包括我在内用了Gemini App之后第一反应：“就这？”但其实他们可能没用到模型能力的一成。你得自己去研究Prompt，自己去摸索AI Studio的用法，才能勉强挖出它的底层潜力。这对99%的用户来说是毫无可能的。

### ChatGPT：产品团队最成熟的一家

相比之下，OpenAI在产品的体验上吊打另外两家。这其实特别反直觉，因为我们提到GPT的时候，第一反应是最老牌的LLM，模型能力业界最强，会下意识的觉得OpenAI主要靠模型来引领竞争，产品可能会没有时间精修。但其实OpenAI模型第一的这个宝座岌岌可危，o3虽然tool use还是顶级，但instruction following的能力还是不如其他两家。context window的长度，多模态的能力（音频和视频理解），和价格也有相当差距。与之相反的是，ChatGPT的产品体验吊打全场，领先其他两家数个身位。它甚至可能是目前唯一可以用到背后AI模型七八成能力的产品。

具体我们来看几个场景：

*   任务异步执行：AI有一个重要场景是，我们在路上使用手机，突然想起来用AI做一些调研。于是我们在app里面输入比如“调研一下XXX”。然后最小化app，把手机熄屏（也可以用杀掉app来模拟）。这时候ChatGPT会继续在后台调研，打开屏幕，重新打开app会发现调研已经做好了，最新的结果就显示在屏幕上。但这个场景Claude会100%失败，这个聊天还能找到，但标题是Untitled，内容为空。Gemini app会大概率失败，整个聊天完全消失，但有小概率这个聊天对话过了一个小时莫名其妙又出现了，里面的内容是正确的。这其实是产品设计思路的区别，只有OpenAI把ChatGPT定位成了一个能在后台帮用户长时间处理任务的工具。Claude虽然在API上强调了这一点，但在消费级产品上并没有贯彻。Gemini的思路也是类似的。
*   iPhone拍照分析照片：如果用户启用了iPhone的Raw拍照的话，拍出来的照片是一个dng文件而不是jpeg或者heic文件。不论是故意的还是无意误触的，这其实是一个非常常见的场景，而且在iphone的相册里面很难看出来差别。如果我们直接上传这个图片的话，Gemini会报错与服务器的连接断开（什么鬼），Claude会报错这个文件类型不支持。虽然不完美，但报错信息至少是对的。但OpenAI就知道先转成jpg，然后上传。这个处理其实非常简单，工程成本很低，做不做完全看产品力，有没有真的去用这个app，踩常见的坑，把细节抠好。
*   巨量文本输入：选中大量文本（比如15万字），粘贴进AI app或者网页。Gemini在按了发送以后会直接卡死，如果你有耐心等个一两分钟可能会恢复。如果没有耐心把手机app放到后台的话，整个聊天像前面测试的一样就消失了。Claude和ChatGPT都会报错说太长了，拒绝处理，但是稍微降低一点文本长度的话可以正常处理。

此外还有很多其他细节，比如能不能在手机端设置system prompt，deep research会不会有live activity的进度更新，个性化的程度有多深等等，就不一一分析了。

不过OpenAI也不是没有问题。比如Web端功能和App端的功能还是有差异，像基于github和sharepoint的deep research，只在Web端支持。此外截止目前还没有MCP支持等等。但从整体上讲，OpenAI是目前唯一把产品设计和模型能力同等重视的公司。体验上没有大的槽点。

### 会不会只是产品还在迭代？

当然，我也不是不理解有些产品会做得克制一点。可能有人会说，Gemini App 没有加视频分析、Claude App 任务中断后不做提示，是因为还在MVP阶段，产品还没来得及做完，战略上选择先把模型上线、用户先跑一跑。这种解释乍一听有道理，但问题是，如果MVP一直持续一年多，核心功能迟迟不上线，连最基础的system prompt、任务不中断、文件上传报正确的错都做不好，那就不是MVP了，而是产品没有被认真对待。战略性克制和资源性敷衍之间，用户是分得清的。

另一个说法是：复杂的功能多数人也用不到，做太多反而压垮产品节奏，保持简洁才是对的。这其实恰好低估了AI产品的本质。AI真正的价值，不在于替代一个搜索引擎或者知识问答工具，而是能够帮用户处理他们自己处理不了或者没时间处理的任务——比如长文档、跨模态素材、复杂规划。如果产品连这些任务都无法承接，那就注定会被用户视为没啥特别的，甚至是鸡肋。

总之，不管任务简单还是复杂，用户都不希望自己的输入白费，更不希望App无声挂掉。这不是高阶功能问题，而是基本的可靠性问题。而现在很多App，连这一点都做不到。

原因与机遇
-----

回头来看，AI模型本身的能力在今天广泛支持不同的生活场景已经不是问题。问题是，类似的模型，被装进了不同的公司、不同的部门、甚至不同的预算流程里之后，最终呈现给用户的样子差别大得离谱。这也是为什么同一个Gemini模型，在AI Studio里表现出了惊人的视频理解和instruction following，但App里笨了很多。这不是技术问题，是组织问题。

我们很可能面对的是不同org做出的两个产品，分别report给不同的VP [[来源]](https://www.semafor.com/article/04/02/2025/google-gemini-shakes-up-ai-leadership-sissie-hsiao-steps-down-replaced-by-josh-woodward)。在这种结构下，Gemini App的产品经理很可能压根不知道模型最大的亮点是什么。他调研了一圈发现ChatGPT和Claude都支持上传图片，但没有支持视频，于是得出结论：那我们也不需要。殊不知视频理解本来就是Gemini最大的优势。(纯猜测，未必真实)

更诡异的是，AI Studio反而做得更好。为什么？因为它是给开发者用的，很多是工程师自己做的，反而离模型更近。你说它是产品，不如说是调试工具。这种没有设计的设计，却比有产品经理但没资源支撑的App版本更好地释放了模型能力。

Claude的问题是另外一种结构问题。它本质是一个To B导向的公司，API才是主营业务，占了85%的收入 [[来源]](https://www.uncoveralpha.com/p/the-hyperscaler-llm-provider-relationships)，To C客户端只是一个“别人有我也得有”的feature parity性质的展示窗口。所以我们看到Claude App就非常随缘：能跑就行，用户断线不提醒，任务跑挂不保存，iOS输出一长直接发烫。没人真正在意用户用它干活，只要能让人做个测试知道它的模型不错就够了。

反过来看OpenAI，它是唯一一家To C 和 To B 两条腿都必须站稳的公司。ChatGPT是它的旗舰产品，占收入的73% [[来源]](https://www.uncoveralpha.com/p/the-hyperscaler-llm-provider-relationships)。更关键的是，它公司小、report chain简单、产品和模型团队捆得紧。你很难想象一个OpenAI产品经理会不知道自己家模型可以识别视频。它能把这套能力接好，只是因为它的组织结构允许它接好。

所以回到我们这篇文章讨论的主题——为什么AI模型变强了，用户却没觉得好用？一个最扎心的答案可能是：不是产品本身难做，而是公司结构的限制。但这也意味着，机会其实还在。

眼下几大模型厂商都在比谁的模型更大、更多模态、更低成本，但真正拼产品体验的，几乎没有。这背后有结构性的障碍，也有路线上的盲点。他们默认模型强了，产品体验自然就会提升；只要能力高，用户就会留下来。这个假设，其实已经被ChatGPT和Gemini App之间的体验落差一定程度上证伪了。

不是所有团队都能把一个能力接好，也不是所有能力都会自动长出好体验。这是一个行业还没被充分讨论的结构性误区，反而给了第三方团队一个非常现实的切入口。

*   如果我们知道Claude 4的模型很稳，但App挂得厉害，那是不是可以接API做一个更稳定的异步任务App？
*   如果我们知道Gemini 2.5在视频分析上吊打全场，但App连视频上传都不支持，那是不是可以干脆用AI Studio的示例代码来包一个轻量客户端，切进垂直市场？
*   如果我们知道所有App都还是聊天框思维，那是不是可以直接跳出对话范式，基于Multi-hour的任务编排来设计一个新的前端结构？

这些都是不靠做模型也能跑通的创新路径。而且它们不是可能有前景的产品，而是现在就存在的用户需求，只是还没人认真做出来。

所以我们回到文章的开头，AI不是不好用，只是大多数人遇到的AI，被封装成了一个错误的形状。模型很聪明，App没跟上。这种体验上的落差，不是技术差距，而是产品设计和组织决策之间长期脱节的结果。

我们今天已经进入了模型不稀缺，体验才稀缺的时代。下一个AI产品的分水岭，也许就藏在你有没有发现这些断层之间的机会。
