# Design Patterns for Securing LLM Agents against Prompt Injections
- URL: https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/
- Added At: 2025-06-14 02:11:23
- [Link To Text](2025-06-14-design-patterns-for-securing-llm-agents-against-prompt-injections_raw.md)

## TL;DR


该论文由IBM等机构提出六大设计模式，通过隔离数据处理、限制代理行为（如禁止访问工具响应、分离规划与执行、双LLM协作等），对抗LLM代理的提示注入攻击。研究指出需在功能与安全间权衡，通过结构化数据处理、最小化上下文等策略降低风险，在医疗、SQL代理等场景验证有效性，为构建安全LLM系统提供实用指南。

## Summary


本文介绍了由IBM、Invariant Labs等机构联合发布的一篇对抗大型语言模型（LLM）代理提示注入攻击的设计模式研究论文，提出了六种核心设计模式，并通过案例研究展示其实用性：

---

### **设计模式**
1. **行动选择器模式**  
   - 代理可触发外部工具（如发送邮件），但无法读取工具响应，避免反馈引发后续风险。

2. **计划-执行模式**  
   - 提前规划工具调用序列，确保工具输出不影响动作选择（例如：日历读取结果可修改邮件内容，但无法更改收件人）。

3. **LLM映射-规约模式**  
   - 通过子代理对未信任数据进行并行处理（如筛选发票文件），结果由协调器安全聚合，减少污染风险。

4. **双LLM模式**  
   - 隔离架构：**特权LLM**（不接触未信任数据）协调**隔离LLM**（处理未信任内容），仅通过符号变量安全传递结果（如$VAR1代表摘要内容）。

5. **代码-执行模式**  
   - 特权LLM生成沙盒DSL代码，定义严格工具调用流程，追踪数据流以标记污染风险（改进自CaMeL论文）。

6. **上下文最小化模式**  
   - 移除历史交互中的用户原始输入（如客服场景），仅保留结构化数据（数据库查询结果），避免提示注入残留。

---

### **关键结论**
- **安全与效用权衡**：所有模式通过限制代理执行范围降低风险，但牺牲部分功能灵活性。
- **核心原则**：一旦代理接触未信任输入，必须严格约束其行为，防止触发有害动作（如泄露敏感信息或破坏系统）。
- **案例验证**：包含SQL代理、医疗诊断聊天机器人等10个实际场景，分析威胁建模及防护策略，例如通过标准化API描述安全消费外部文档。

---

### **研究价值**
- 承认LLM代理无法提供绝对安全保证，但提出可行路径在“有用”与“安全”间达成平衡。
- 强调设计模式的渐进性贡献，为构建当前技术条件下的安全代理提供实用指南。
