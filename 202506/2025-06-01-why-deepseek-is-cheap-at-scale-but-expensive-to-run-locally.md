# Why DeepSeek is cheap at scale but expensive to run locally
- URL: https://www.seangoedecke.com/inference-batching-and-deepseek/
- Added At: 2025-06-01 04:16:47
- [Link To Text](2025-06-01-why-deepseek-is-cheap-at-scale-but-expensive-to-run-locally_raw.md)

## TL;DR


文章指出，DeepSeek-V3等模型的大规模部署成本低，但本地运行效率低，因GPU依赖批量矩阵运算提升效率。其MoE架构需大批次避免专家层资源浪费，深层流水线则易因小批量产生空闲，本地低并发难以形成有效批量，加剧GPU利用率低下问题。服务端以高延迟换取大批次处理（如200ms窗口），实现高吞吐低成本，而本地缺乏此条件导致成本高昂。

## Summary


文章探讨了为何DeepSeek-V3等模型在大规模部署时成本低廉但本地运行效率低下，核心原因涉及GPU计算特性、模型架构及批量处理（batching）策略的权衡。以下为结构化总结：

---

### **核心原因：GPU效率与批量处理的权衡**
- **GPU特性**：GPU擅长**大规模矩阵乘法（GEMM）**，处理一批（batch）token的效率远高于逐个处理。例如，处理10个token的矩阵乘法仅需一次大运算，而逐个处理需10次小运算，效率显著降低。
- **吞吐量与延迟的平衡**：
  - **高吞吐量**：通过**批量合并多个用户请求**（跨数十至数百个并发请求）提升GPU利用率，但需等待足够多请求形成批量，导致**高延迟**。
  - **低延迟**：逐个处理请求可减少等待时间，但GPU效率低下，需更多硬件资源。

---

### **模型特性决定高批量需求**
1. **混合专家模型（MoE）**：
   - MoE模型包含**数百个专家层**，每层需独立计算。若批量过小，部分专家可能仅处理少量token，导致**GPU计算资源浪费**。
   - **解决方案**：需**大批次**（如4000个请求）确保所有专家被充分使用，但需接受更高延迟。

2. **深层模型与流水线问题**：
   - 大型模型需**多GPU流水线**（逐层处理）。若批次小，各GPU层可能出现**空闲时间（管道气泡）**，因后续层需等待前层完成数据传递。
   - **大批次**可填充各GPU层队列，减少空闲时间，但增加延迟。

---

### **DeepSeek-V3的特殊性**
- **双重挑战**：
  - 结合**MoE架构**与**深层结构**，对高批量依赖度极高。
  - 本地运行时缺乏并发请求，无法形成有效批量，导致GPU利用率极低，成本高昂。
- **部署策略**：
  - 服务端通过**延长批量收集窗口**（如200ms），合并足够多请求以避免管道气泡并饱和专家层，牺牲延迟换取高吞吐量。

---

### **其他模型的可能优化**
- **高效架构**：如非MoE模型或较少层数，可降低对批量大小的依赖，实现低延迟。
- **技术手段**：通过优化注意力机制的批次处理（允许不同序列长度混合）或先进调度算法，可能平衡吞吐量与延迟。
- **硬件投入**：如OpenAI可能通过部署更多GPU，以高成本换取低延迟，但大规模部署成本仍具竞争力。

---

### **结论**
- **规模效应驱动成本差异**：大规模部署时，高批量策略通过最大化GPU利用率降低成本，但本地低并发场景下无法实现类似效率。
- **架构选择的影响**：MoE和深层模型迫使系统依赖高批量，成为其本地运行成本高昂的关键原因。
