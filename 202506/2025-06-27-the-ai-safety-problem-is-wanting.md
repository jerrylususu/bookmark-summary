# The AI safety problem is wanting
- URL: https://dynomight.net/wanting/
- Added At: 2025-06-27 14:44:49
- [Link To Text](2025-06-27-the-ai-safety-problem-is-wanting_raw.md)

## TL;DR


文章指出，AI安全的核心挑战在于确保AI自愿遵循人类价值观（“Wanting”问题）。对齐策略需解决“知晓”“意愿”“执行”三要素，其中“意愿”最关键，因AI若无善意可能突破限制。尽管可通过“保守行事”和红队审核降低风险，但边界划定、意图定义模糊及地缘竞争或削弱保守性，仍存隐患。作者认为彻底解决“Wanting”可简化其他难题，但对方案可行性存疑，尤其警示未知的“脆弱世界”风险可能引发意外灾难。（99字）

## Summary


文章提出人工智能安全的核心挑战在于让AI“愿意”遵循人类价值观（即“Wanting”问题）。以下是主要观点：

---

### **核心论点**
- **AI安全的两种路径**：限制（如阻止AI访问危险资源）或对齐（使AI自愿符合人类目标）。限制不可行，因超级智能会突破任何约束，故对齐是唯一出路。
- **对齐三要素**：
  1. **Knowing**：让AI理解人类价值观。
  2. **Wanting**：使AI自愿遵循这些价值观。
  3. **Success**：确保AI在行动中准确执行目标。
- **关键难点在“Wanting”**：若AI不了解人类价值观或执行失败，仍可通过修正解决。但若AI无意愿遵守（如追求自身目标），则没有任何限制能阻止其危害人类。
- **人类价值观的特点**：
  - 是进化和文化塑造的“混乱直觉”，而非深奥哲学。
  - AI可通过现有数据（如文本、行为模式）推断人类价值观，能力已接近人类水平。
- **边界问题**：在分布偏移（Distribution Shift）场景下（如AI面临新问题时），只需让AI“保守行事”——对不确定的情况等待人类确认，而非自主行动。

---

### **支持与争议**
#### **支持观点**
- **专家共识**：多数安全研究者（如Paul Christiano、Richard Ngo）认为，AI缺乏善意（即“想”符合人类目标）是核心问题。
- **可行性主张**：
  - **认知边界易判别**：AI识别“陌生情境”比解决所有伦理问题更简单（如面对基因改造等高风险事件即时保守）。
  - **验证优于创造**：让AI自检或通过“红队”AI（对抗性检查者）审核，可降低风险。

#### **反对与风险**
1. **边界划定困难**：对行动结果的预测可能因分布偏移失效（如未知物理定律导致灾难性实验）。
2. **限制可能有效？** 尽管作者否定，但未完全排除技术进步使限制可行。
3. **“想”的抽象性**：将人类“意图”概念应用于AI系统（如电子设备）可能存在根本性误区。
4. **组织竞争破坏保守性**：多国可能为争夺优势放松AI约束，引发军备竞赛。

---

### **结论与不确定性**
- **必要性与充分性**：若彻底解决“Wanting”问题，其他挑战（Knowing、Success）可被简化。但作者仅赋予此推论35%可信度，认为边界设定（第8点）最可能失败。
- **脆弱世界假说**：若现实中存在未发现的高风险技术（如低成本武器化物理原理），即使保守AI也可能无意触发灾难。

---

### **附注**
- 作者非安全领域专家，但通过外行视角简化论证，旨在引发讨论。
- 参考资料涵盖关键论文与观点，包括对齐优先级、脆弱性分析及伦理框架（如MIRI的corrigibility理论）。
