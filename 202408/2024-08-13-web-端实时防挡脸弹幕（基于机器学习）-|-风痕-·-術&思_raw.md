Title: Web 端实时防挡脸弹幕（基于机器学习） | 风痕 · 術&思

URL Source: https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/

Markdown Content:
[_W3C 分享视频版_ (opens new window)](https://www.bilibili.com/video/BV12P411p7He/)

**防档弹幕**，即大量弹幕飘过，但不会遮挡视频画面中的人物，看起来像是从人物背后飘过去的。  
![Image 1](https://hughfenghen.github.io/assets/img/bili-mask-danmaku.d2c8b546.jpeg)

[#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E5%89%8D%E8%A8%80) 前言
--------------------------------------------------------------------------------------------

![Image 2: mediapipe 示例](https://hughfenghen.github.io/assets/img/mediapipe.f6877661.gif)  
[mediapipe (opens new window)](https://google.github.io/mediapipe/)demo 展示

机器学习已经火了好几年了，但很多人的直觉仍然是前端实现不了这些能力，期望本文能打破一些“思维禁区”。

### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E4%B8%BB%E6%B5%81%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D) 主流实现原理介绍

#### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E7%82%B9%E6%92%AD) 点播

1.  up 上传视频
2.  服务器后台计算提取视频画面中的人像区域，转换成 svg 存储
3.  客户端播放视频的同时，从服务器下载 svg 与弹幕合成，人像区域不显示弹幕

#### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E7%9B%B4%E6%92%AD) 直播

1.  主播推流时，实时（主播设备）从画面提取人像区域，转换成 svg
2.  将 svg 数据合并到视频流中（SEI），推流至服务器
3.  客户端播放视频同时，从视频流中（SEI）解析出 svg
4.  将 svg 与弹幕合成，人像区域不显示弹幕

### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E6%9C%AC%E6%96%87%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88) 本文实现方案

1.  客户端播放视频同时，实时从画面提取人像区域信息
2.  将人像区域信息导出成图片，与弹幕合成，人像区域不显示弹幕

#### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86) 实现原理

1.  采用机器学习开源库从视频画面实时提取人像轮廓，如[Body Segmentation (opens new window)](https://github.com/tensorflow/tfjs-models/blob/master/body-segmentation/README.md)
2.  将人像轮廓转导出为图片，设置弹幕层的 [mask-image (opens new window)](https://developer.mozilla.org/zh-CN/docs/Web/CSS/mask-image)

![Image 3](https://hughfenghen.github.io/assets/img/body-mask.a0808bd3.png) ![Image 4](https://hughfenghen.github.io/assets/img/compsite.e5d4f24d.png)

#### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E9%9D%A2%E4%B8%B4%E7%9A%84%E9%97%AE%E9%A2%98) 面临的问题

众所周知“_JS 性能太辣鸡_”，不适合执行 CPU 密集型任务。  
由官方 demo 转换成工程实践，并非调一下 API 就行了，最大的挑战就是——**性能**。

一开始我也不敢相信实时计算，能将 **CPU 占用优化到 5% 左右**（2020 M1 Macbook）  
甚至低于主流实现中，单在客户端上的性能损耗（解析 svg，与弹幕合成）

* * *

\------------------------------ 正片开始，以下是调优过程 ------------------------------

* * *

[#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E9%80%89%E6%8B%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B) 选择机器学习模型
--------------------------------------------------------------------------------------------------------------------------------------------------------

可展开[BodyPix (opens new window)](https://github.com/tensorflow/tfjs-models/blob/master/body-segmentation/src/body_pix/README.md) **X**  
精确度太差，有很明显的弹幕与面部重合现象

![Image 5](https://hughfenghen.github.io/assets/img/bodypix-mask.b283275f.png) 可展开[BlazePose (opens new window)](https://github.com/tensorflow/tfjs-models/blob/master/pose-detection/src/blazepose_mediapipe/README.md)**X**  
精确度跟后面的 MediaPipe SelfieSegmentation 差不多，因为提供了肢体点位信息，**CPU 占用相对高出 15% 左右**

![Image 6](https://hughfenghen.github.io/assets/img/blacepose-mask.0c65af60.png) 可展开[MediaPipe SelfieSegmentation (opens new window)](https://github.com/tensorflow/tfjs-models/blob/master/body-segmentation/src/selfie_segmentation_mediapipe/README.md) **√**  
精确度优秀，只提供了人像区域信息，性能取胜

![Image 7](https://hughfenghen.github.io/assets/img/bodysegment-mask.f526046f.png)

* * *

参考[官方实现 (opens new window)](https://github.com/tensorflow/tfjs-models/blob/master/body-segmentation/README.md#bodysegmentationdrawmask)，未做优化的情况下 **CPU 占用 70% 左右**

[#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E9%99%8D%E4%BD%8E%E6%8F%90%E5%8F%96%E9%A2%91%E7%8E%87-%E5%B9%B3%E8%A1%A1-%E6%80%A7%E8%83%BD-%E4%BD%93%E9%AA%8C) 降低提取频率，平衡 性能-体验
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

一般视频 30FPS，尝试弹幕遮罩（后称 Mask）刷新频率降为 15FPS，体验上还能接受  
_（再低就影响体验了）_

此时，**CPU 占用 50% 左右**

[#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E8%A7%A3%E5%86%B3%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88%E4%BB%A3%E7%A0%81) 解决性能瓶颈代码
--------------------------------------------------------------------------------------------------------------------------------------------------------

![Image 8](https://hughfenghen.github.io/assets/img/flame-graph.5f5ace85.png)

分析火焰图可发现，性能瓶颈在 `toBinaryMask` 和 `toDataURL`

### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E9%87%8D%E5%86%99-tobinarymask) 重写 toBinaryMask

分析源码，结合打印`segmentation`的信息，发现`segmentation.mask.toCanvasImageSource`可获取原始`ImageBitmap`对象，即是模型提取出来的信息。  
尝试自行实现将`ImageBitmap`转换成 Mask 的能力，替换开源库提供的默认实现。

**实现原理**

第 2、3 步相当于给人像区域外的内容填充黑色（反向填充`ImageBitmap`），是为了配合 css（mask-image）， 不然只有当弹幕飘到人像区域才可见（与目标效果正好相反）。  
[globalCompositeOperation MDN (opens new window)](https://developer.mozilla.org/zh-CN/docs/Web/API/CanvasRenderingContext2D/globalCompositeOperation)

![Image 9](https://hughfenghen.github.io/assets/img/css-mask-expect.0ea04d5e.png)

此时，**CPU 占用 33% 左右**

### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%BC%98%E5%8C%96) 多线程优化

只剩下`toDataURL`这个耗时操作了，本以为`toDataURL`是浏览器内部实现，无法再进行优化了。  
虽没有替换实现，但可使用 [OffscreenCanvas (opens new window)](https://developer.mozilla.org/zh-CN/docs/Web/API/OffscreenCanvas) + Worker，将耗时任务转移到 Worker 中去， 避免占用主线程，就不会影响用户体验了。  
并且`ImageBitmap`实现了`Transferable`接口，可被转移所有权，[跨 Worker 传递也没有性能损耗](https://hughfenghen.github.io/_posts/fe-basic-course/js-concurrent.html#%E4%B8%A4%E4%B8%AA%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94)。

![Image 10](https://hughfenghen.github.io/assets/img/flame-graph-2.87a27c93.png)

可以看到两个耗时的操作消失了  
此时，**CPU 占用 15% 左右**

[#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E9%99%8D%E4%BD%8E%E5%88%86%E8%BE%A8%E7%8E%87) 降低分辨率
--------------------------------------------------------------------------------------------------------------------------

继续分析，上图重新计算样式（紫色部分）耗时约 3ms  
Demo 足够简单很容易推测到是这行代码导致的，发现 `imgStr` 大概 100kb 左右（视频分辨率 1280x720）。

针对视频弹幕的应用场景，需要是人像的轮廓信息，对 Mask 图片的质量和尺寸要求很低，  
所以，可以降低给推理模型的图像的分辨率，来大幅减少计算量，  
对输出的 Mask 图片拉伸到原尺寸，再与弹幕层合成。

既可以大幅降低计算量，又不会损失体验。

![Image 11: reduce-resolution](https://hughfenghen.github.io/assets/img/reduce-resolution.c94d6adc.png)

**优化实现**

优化后，导出的 `imgStr` 大概 12kb，重新计算样式耗时约 0.5ms。  
此时，**CPU 占用 5% 左右**

![Image 12](https://hughfenghen.github.io/assets/img/cpu.421e0e6e.gif)

[#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E5%90%AF%E5%8A%A8%E6%9D%A1%E4%BB%B6%E4%BC%98%E5%8C%96) 启动条件优化
------------------------------------------------------------------------------------------------------------------------------------

虽然提取 Mask 整个过程的 CPU 占用已优化到可喜程度。  
当在画面没人的时候，或没有弹幕时候，可以停止计算，实现 0 CPU 占用。

_无弹幕判断比较简单（比如 10s 内收超过两条弹幕则启动计算），也不在该 SDK 实现范围，略过_

**判定画面是否有人**  
第一步中为了高性能，选择的模型只有`ImageBitmap`，并没有提供肢体点位信息。  
所以只能使用`ImageBitmap`来判断是否有人。  
画面中的人物大概率是画面**中间且是连续的区域**，所以从中间开始往左右反复横跳检查像素值，碰到一个 alpha 通道为零的像素点就表示画面有人。  
![Image 13](https://hughfenghen.github.io/assets/img/detect-person.d1df1a73.png)

画面无人时，**CPU 占用接近 0%**

[#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E5%8F%91%E5%B8%83%E6%9E%84%E5%BB%BA%E4%BC%98%E5%8C%96) 发布构建优化
------------------------------------------------------------------------------------------------------------------------------------

依赖包的体积较大，构建出的 bundle 体积：`684.75 KiB / gzip: 125.83 KiB`

所以，可以进行异步加载 SDK，提升页面加载性能。

1.  分别打包一个 loader，一个主体
2.  由业务方 import loader，首次启用时异步加载主体

这个两步前端工程已经非常成熟了，略过细节。

[#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E6%80%BB%E7%BB%93) 总结
--------------------------------------------------------------------------------------------

### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E8%BF%87%E7%A8%8B) 过程

*   选择高性能模型后，初始状态 CPU 70%
*   降低 Mask 刷新频率（15FPS），CPU 50%
*   重写开源库实现（toBinaryMask），CPU 33%
*   多线程优化，CPU 15%
*   降低分辨率，CPU 5%
*   判断画面是否有人，无人时 CPU 接近 0%

_CPU 数值指主线程占用_

### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9) 注意事项

*   **兼容性**：Chrome 79 及以上，不支持 Firefox、Safari。因为使用了`OffsccenCanvas`
*   不应创建多个或多次创建`segmenter`实例（bodySegmentation.createSegmenter），如需复用请保存实例引用，因为：
    *   创建实例时低性能设备会有明显的卡顿现象
    *   会内存泄露；如果无可避免，这是[mediapipe 内存泄露 解决方法 (opens new window)](https://github.com/google/mediapipe/issues/2819#issuecomment-1160335349)

### [#](https://hughfenghen.github.io/posts/2023/06/21/body-mask-danmaku/#%E7%BB%8F%E9%AA%8C) 经验

*   结合业务场景特征进行分析优化，往往有更多的思路和途径
*   优化完成之后，提取并应用 Mask 关键计算量在 GPU (30%左右)，而不是 CPU
*   性能优化需要业务场景分析，防档弹幕场景可以使用低分辨率、低刷新率的 mask-image，能大幅减少计算量
*   该方案其他应用场景：
    *   替换/模糊人物背景
    *   人像马赛克
    *   人像抠图
    *   卡通头套，虚拟饰品，如猫耳朵、兔耳朵、带花、戴眼镜什么的（换一个模型，略改）
*   关注 [WebNN (opens new window)](https://www.w3.org/TR/webnn/)、[WebGPU (opens new window)](https://developer.mozilla.org/zh-CN/docs/Web/API/WebGPU_API) 的进展和应用，端智能将覆盖更多的应用场景
