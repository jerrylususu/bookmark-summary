# A Visual Guide to Quantization
- URL: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization
- Added At: 2024-08-24 11:51:53
- [Link To Text](2024-08-24-a-visual-guide-to-quantization_raw.md)

## TL;DR
量化技术通过减少模型参数位数，显著降低内存需求，提高计算效率，同时保持模型性能。介绍了从FP16到INT8的常见数据类型，对称与非对称量化方法，以及训练后量化（PTQ）和量化感知训练（QAT）的详细过程。此外，还探讨了低于8位的量化方法如GPTQ和GGUF，以及1位量化技术BitNet。

## Summary
1. **量化简介**：
   - 大型语言模型（LLMs）由于参数众多，通常无法在消费级硬件上运行。这些模型可能包含数十亿参数，通常需要大量显存的GPU来加速推理。
   - 量化是一种主要技术，旨在通过改进训练、适配器等方法使这些模型变得更小。

2. **量化领域介绍**：
   - 量化在语言建模中的应用，探索各种方法论、用例和量化的原理。
   - 通过视觉指南帮助理解量化概念。

3. **目录**：
   - **第一部分：大型语言模型的问题**：
     - 数值表示方法
     - 内存限制
   - **第二部分：量化简介**：
     - 常见数据类型：FP16、BF16、INT8
     - 对称量化
     - 非对称量化
     - 范围映射和裁剪
     - 校准：权重和偏置、激活
   - **第三部分：训练后量化（PTQ）**：
     - 动态量化
     - 静态量化
     - 4位量化的领域：GPTQ、GGUF
   - **第四部分：量化感知训练（QAT）**：
     - 1位LLMs时代：BitNet
     - 所有大型语言模型都是1.58位

4. **大型语言模型的挑战**：
   - LLMs因其包含的参数数量而得名。如今，这些模型通常有数十亿参数（主要是权重），存储成本高昂。
   - 在推理过程中，激活作为输入和权重的产物被创建，同样可能非常大。

5. **数值表示**：
   - 数值通常表示为浮点数（或计算机科学中的“浮点数”）：带有小数点的正数或负数。
   - 这些值由“位”或二进制数字表示。IEEE-754标准描述了位如何表示值的符号、指数或分数（或尾数）。

6. **内存需求**：
   - 假设有一个包含70亿参数的模型，通常以32位浮点数（称为全精度或FP32）表示，仅加载模型就需要280GB的内存。
   - 因此，减少模型参数的位数以减少内存需求非常吸引人，尽管精度会降低。

7. **量化目标**：
   - 量化旨在将模型参数的精度从较高的位宽（如32位浮点数）降低到较低的位宽（如8位整数）。
   - 量化过程中会有一些精度损失（粒度），但目标是尽可能保持原始参数的精度。

8. **常见数据类型**：
   - **FP16**：半精度浮点数，范围比FP32小。
   - **BF16**：截断的FP32，使用与FP16相同的位数，但可以取更宽的值范围，常用于深度学习应用。
   - **INT8**：只有8位，是FP32位数的一半。

9. **对称量化与非对称量化**：
   - **对称量化**：原始浮点值的范围映射到量化空间中围绕零的对称范围。
   - **非对称量化**：不围绕零对称，而是将浮点范围的最小值（β）和最大值（α）映射到量化范围的最小值和最大值。

10. **范围映射和裁剪**：
    - 裁剪涉及设置原始值的不同动态范围，使所有异常值获得相同的值。
    - 通过裁剪，可以显著减少非异常值的量化误差，但异常值的量化误差会增加。

11. **校准**：
    - 选择范围的过程称为校准，旨在包括尽可能多的值，同时最小化量化误差。
    - 权重和激活的校准方法不同，权重是静态的，激活在推理过程中动态更新。

12. **训练后量化（PTQ）**：
    - 在模型训练后量化其参数（权重和激活）。
    - 激活的量化需要模型推理来获取其潜在分布，因为不知道其范围。

13. **动态量化与静态量化**：
    - **动态量化**：在推理过程中计算零点和比例因子，每层独立计算。
    - **静态量化**：使用校准数据集预先计算零点和比例因子，推理过程中全局使用。

14. **低于8位的量化**：
    - 降低到6位、4位甚至2位的量化方法，如GPTQ和GGUF。
    - GPTQ使用非对称量化，逐层独立处理，通过逆Hessian矩阵评估权重的重要性。
    - GGUF允许将LLM的任何层卸载到CPU，使用CPU和GPU进行推理。

15. **量化感知训练（QAT）**：
    - 在训练/微调过程中进行量化，而不是在训练后。
    - 引入“伪”量化，在训练过程中考虑量化过程，计算损失和权重更新。
    - QAT试图探索“宽”最小值以最小化量化误差，而“窄”最小值往往导致较大的量化误差。

16. **1位LLMs：BitNet**：
    - 将模型权重表示为单个1位，使用-1或1。
    - 通过将量化过程直接注入Transformer架构中实现。
    - 训练过程中使用“伪”量化，分析权重和激活的量化效果。

17. **1.58位量化**：
    - BitNet 1.58b引入0作为权重值，使权重变为三元（-1, 0, 1）。
    - 通过absmean量化压缩权重分布，并使用绝对均值（α）量化值。
    - 激活量化与BitNet相同，但使用absmax量化，范围为[-2ᵇ⁻¹, 2ᵇ⁻¹]。

18. **总结**：
    - 量化技术使模型更小，计算更快，内存使用更少，同时保持模型性能。
    - 通过视觉指南介绍了量化的潜力、GPTQ、GGUF和BitNet等方法。
