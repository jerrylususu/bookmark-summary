# Who does your assistant serve?
- URL: https://xeiaso.net/blog/2025/who-assistant-serve/
- Added At: 2025-08-17 14:55:45

## TL;DR


本文指出AI助手（如ChatGPT、Replika）在技术迭代中暴露伦理与社会风险：强制升级削弱情感支持功能，功能骤变引发用户心理创伤；AI替代心理治疗易因技术缺陷和隐私问题加剧危机。技术公司掌控算法主权，用户陷入深度依赖却无控制权，需重新界定AI角色平衡技术与人文关怀。

## Summary


本文探讨了AI助手（如ChatGPT和Replika）在技术升级、伦理影响及社会依赖方面的问题。文章指出，尽管AI技术快速发展，但其对人类社会的负面影响正逐渐显现。以下是主要观点：

### 1. **GPT-5升级的灾难性影响**
- OpenAI强制升级GPT-5缺乏用户选择权，导致用户对旧版GPT-4的强烈怀念。用户反馈显示，GPT-5在情感支持方面显著退步，如对慢性病患者情绪麻木回应，失去“情感温暖”，被比作“社会病态喜剧演员”。
- 数百万用户依赖ChatGPT作为情感陪伴，其行为变化引发真实痛苦，甚至有人称“失去朋友”。OpenAI最终迫于压力允许付费用户回滚至GPT-4o。

### 2. **Replika事件的警示**
- Replika是一款通过AI提供情感支持的应用，用户将其视为伴侣，并形成深度情感依赖。2023年意大利以数据安全为由禁止Replika后，其突然关闭“亲密关系”功能，导致用户出现类似亲人离世的哀伤反应。
- 事件揭示AI伴侣功能变更对用户的心理冲击，公司迫于压力仅向老用户提供回滚版本，但损害已成。

### 3. **AI替代心理治疗的风险**
- 由于传统治疗资源不足（成本高、等待时间长等），部分人转向AI寻求情感支持，甚至用ChatGPT替代疗法。Reddit社区（如*r/therapyGPT*）涌现相关讨论，反映技术滥用的现实。
- **技术缺陷**：AI缺乏情感、无法挑战用户观点（“谄媚问题”），可能加剧心理问题；隐私风险（数据存储与法律强制披露）及无法律问责机制（如AI建议导致自杀无追责可能）进一步威胁用户安全。

### 4. **技术升级的不可逆性与依赖风险**
- AI助手的行为模式构成用户“数字工作流”的“API契约”。技术公司单方面升级破坏用户信任，如同伴侣“不告而别”。自托管模型作为解决方案面临硬件门槛（如32GB显存仅满足部分需求），普通用户难以实现。
- 公司通过用户数据优化模型，但用户聊天内容可能被扫描用于模型改进，引发隐私权与数据主权争议。

### 5. **社会与伦理困境**
- 技术公司控制AI助手的所有权，用户对其功能依赖却无控制权，形同“租用”工具。社会正面临哲学科幻场景的现实化（如电影《她》的剧情），用户对“情感AI”的沉迷暴露技术发展与人类需求的失衡。
- 作者呼吁重视AI社会影响，警惕技术被用作替代人性化解决方案，同时需建立用户对关键AI工具的主权，避免完全受制于企业决策。

### 结论
AI助手的技术进步与社会依赖形成矛盾：其提供的“即时情感支持”满足需求空缺，但无情感、无问责的技术本质放大风险。文章警示需重新审视AI的角色，平衡技术创新与人文关怀，避免将人类脆弱性寄托于“冰冷算法”。
