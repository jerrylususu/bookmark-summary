# CaMeL offers a promising new direction for mitigating prompt injection attacks
- URL: https://simonwillison.net/2025/Apr/11/camel/
- Added At: 2025-04-12 06:14:26
- [Link To Text](2025-04-12-camel-offers-a-promising-new-direction-for-mitigating-prompt-injection-attacks_raw.md)

## TL;DR


Google DeepMind提出CaMeL系统，通过将用户指令转化为结构化代码并结合受限解释器与能力标签机制，有效防御提示注入攻击。其核心采用变量溯源及动态策略控制，弥补传统Dual-LLM模式缺陷，具备隐私保护与低成本优势。但需手动定义安全策略，仍面临复杂攻击风险。该方案创新性地将系统工程方法应用于AI安全领域，为构建可信智能助手提供新路径。

## Summary


该文章介绍了Google DeepMind提出的新系统CaMeL（Capabilities for MachinE Learning），旨在有效解决人工智能系统中的提示注入攻击问题。以下是核心内容总结：

1. **问题背景**  
   提示注入攻击利用语言模型将用户指令与不可信文本（如邮件、网页）混合处理的特性，可能导致系统执行恶意操作（如泄露数据）。传统防御方法（如Dual-LLM模式）虽部分有效，仍存在漏洞：若隔离的Q-LLM（非特权模型）被诱导篡改关键数据（如收件人邮箱），仍可能引发数据外泄。

2. **CaMeL的核心机制**  
   - **指令转化为结构化代码**：用户指令被转化为Python脚本形式的步骤序列（如检索邮件、提取地址、发送文档），借助LLM生成代码。
   - **定制解释器**：采用受限Python环境，逐层解析并执行代码，实时追踪变量来源及安全性。
   - **能力标签与策略控制**：为变量附加“能力标签”（如数据来源可靠性），依据安全策略限制操作（如仅允许向可信地址发送文件）。

3. **关键创新与优势**  
   - **系统化设计而非依赖AI**：突破传统依赖模型训练的方法，通过变量溯源、权限控制等安全工程手段实现“强安全保证”，避免概率性防御方案的漏洞。
   - **隐私保护**：Q-LLM可采用小型本地模型处理敏感数据，减少云端数据泄露风险，同时降低成本。
   - **改进Dual-LLM缺陷**：通过代码解析和动态策略，解决原模式中隔离模型可能被操控的问题。

4. **局限性与挑战**  
   - **用户负担**：需手动定义安全策略，可能引发决策疲劳（如频繁确认操作），系统需平衡安全与用户体验。
   - **尚未完全解决风险**：复杂攻击可能绕过现有控制，仍需持续优化策略设计与执行流程。

5. **意义与展望**  
   CaMeL是首个基于系统工程而非堆叠AI模型的防御方案，整合了能力标签、数据流分析等经典安全概念，为构建安全可靠的智能助手提供了可行性路径。其成功或依赖预设安全策略的易用性改进及用户界面设计优化。
