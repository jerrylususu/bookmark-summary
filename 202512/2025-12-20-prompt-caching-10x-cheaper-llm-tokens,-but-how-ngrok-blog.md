# Prompt caching: 10x cheaper LLM tokens, but how? | ngrok blog
- URL: https://ngrok.com/blog/prompt-caching/
- Added At: 2025-12-20 11:06:23
- Tags: #read #llm #explainer
- [Link To Text](2025-12-20-prompt-caching-10x-cheaper-llm-tokens,-but-how-ngrok-blog_raw.md)

## TL;DR
文章介绍了提示缓存如何通过复用语言模型的K和V矩阵，避免重复计算输入令牌，从而降低成本90%并减少延迟。OpenAI和Anthropic的缓存策略不同，但均显著提升效率，适用于长提示场景。

## Summary
这篇文章详细解释了提示缓存的机制及其带来的成本和延迟优势。

### 核心要点
- **提示缓存的优势**：OpenAI和Anthropic的API中，缓存输入令牌成本降低90%（10倍），延迟减少高达85%，尤其适用于长提示。
- **缓存本质**：缓存的是LLM注意力机制中的K（键）和V（值）矩阵，而非完整响应，避免重复计算，从而提升效率。

### LLM工作原理概述
1. **分词器**：将文本分割成令牌（整数ID），处理语义和大小写差异。
2. **嵌入层**：将令牌转换为高维向量（嵌入），表示语义含义，并编码位置信息。
3. **变换器层**：核心是注意力机制，通过矩阵运算（如Q、K、V）计算令牌间权重，混合嵌入以生成上下文感知输出。
   - 注意力权重决定每个令牌对生成新令牌的影响程度。

### 提示缓存具体机制
- **问题**：标准推理循环中，每次生成新令牌都需重新计算整个提示的K和V矩阵，效率低下。
- **解决方案**：
  - 缓存每次迭代的K和V矩阵。
  - 新请求时，仅处理新增令牌，复用缓存的K和V，避免重复计算。
- **结果**：大幅减少计算量，实现成本节约和延迟降低。
- **缓存策略**：
  - OpenAI：自动路由请求至缓存条目，命中率约50%。
  - Anthropic：用户可控缓存时长，命中率100%，适合需要可预测延迟的场景。
- **不影响参数**：温度（temperature）等随机性参数仅影响输出阶段，不干扰缓存有效性。

### 结论
提示缓存通过优化注意力机制的计算流程，显著提升LLM效率。文章末尾推广了ngrok.ai产品，用于统一管理LLM流量。
