# A pragmatic guide to LLM evals for devs
- URL: https://newsletter.pragmaticengineer.com/p/evals
- Added At: 2025-12-03 14:40:45
- Tags: #read #llm #eval #deepdive

## TL;DR
本文总结了LLM应用开发中的评估核心流程：通过错误分析系统化识别主要失败模式（如构建数据查看器、开放式与轴向编码），结合代码化测试和LLM评判员工具，实现数据驱动的持续优化，取代主观开发模式。

## Summary
本文主要介绍了针对开发者在构建基于大语言模型解决方案时如何高效进行评估的方法，强调了评估的核心作用和实践流程。

**评估的重要性与挑战**
- 由于LLM的非确定性特性（相同的输入可能产生不同输出），传统自动化测试方法不适用，评估成为验证LLM解决方案的关键工具。
- 评估正集成到CI/CD流程中，开发人员需掌握评估方法以避免“基于感觉的开发陷阱”（即仅凭主观判断发布模型修改）。

**核心评估方法：错误分析**
- 错误分析是机器学习中的成熟方法，适用于LLM应用开发。通过系统化流程识别和优先处理失败模式：
  1. **构建简易数据查看器**：定制化工具可高效展示交互轨迹（trace），便于快速审查。
  2. **开放式编码**：人工审查至少100条轨迹，记录开放式问题笔记，聚焦首个上游失败点以避免冗余。
  3. **轴向编码**：将问题笔记归类为5-10个主题，利用LLM辅助聚类后人工细化。
  4. **数据驱动优先级**：通过频次统计（如数据透视表）定量确定修复重点。
- 若缺乏真实用户数据，可使用合成数据模拟场景启动分析。

**评估工具选择**
- **代码化评估**：适用于确定性任务（如日期解析），通过“黄金数据集”进行断言测试，成本低且易集成到CI/CD。
- **LLM作为评判员**：针对主观性问题（如对话移交判断），需构建包含PASS/FAIL标签和详细批评的数据集，避免使用模糊的评分制。

**LLM评判员的构建与验证**
- 利用领域专家标注的数据训练LLM评判员，以实现自动化评估。
- 需通过数据分区防止评判员记忆答案，并验证其与人类专家的一致性（如使用真阳性率等指标）。
- 评估应覆盖从CI/CD到生产监控的全流程，形成“分析→测量→改进→自动化”的持续优化飞轮。

**实践案例**
- 以NurtureBoss为例，通过定制工具和错误分析，团队将问题归结为日期处理、移交失败等关键类别，从而针对性优化，取代了主观开发模式。

总之，评估是确保LLM应用质量的核心工程实践，需结合系统化错误分析和合适的工具选择，实现从感觉驱动到数据驱动的转变。
