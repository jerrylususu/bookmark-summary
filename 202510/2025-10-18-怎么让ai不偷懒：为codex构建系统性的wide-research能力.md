# 怎么让AI不偷懒：为Codex构建系统性的Wide Research能力
- URL: https://grapeot.me/wide-research.html
- Added At: 2025-10-18 14:03:02
- [Link To Text](2025-10-18-怎么让ai不偷懒：为codex构建系统性的wide-research能力_raw.md)

## TL;DR
本文通过分析AI处理长任务时“偷懒”的架构性问题，提出分治策略：将大规模任务分解为子问题由轻量级AI处理，再汇总润色。作者以Codex为例实现自动化工答流程，展示了改进AI系统设计和工作流对提升执行效率的关键作用。

## Summary
这篇文章探讨了如何通过改进AI架构和工作流程来解决AI在处理大规模任务时“偷懒”的问题，并以Codex为例构建了系统化的Wide Research能力。以下从问题背景、原因分析、解决方案和关键实践四个方面进行结构化总结。

### 问题背景
- **任务示例**: 邹老师需要总结53篇学生博客文章，但现有AI工具（如DeepSeek、Claude、GPT等）在处理此任务时普遍“偷懒”，仅能总结少量文章（最多约20篇），远低于目标数量。
- **成功案例**: 使用Manus的Wide Research功能或作者在Codex上复现的类似方案（开源Repo），最终实现了完整、准确且深入的总结，证明该方法的有效性。

### AI“偷懒”的原因分析
- **架构性约束**: 所有基于Transformer的LLM在输出长度达到context window的一定比例（如20%-50%）时，指令跟随能力会下降，导致任务中途放弃。这是因为全局注意力机制的限制，输出规模增大会分散模型注意力。
- **执行力差异**: 不同LLM在偷懒起始点（如GPT-4o-mini从几百字开始，GPT-5可支撑数千字）和工具使用习惯（如Gemini爱搜索、Codex爱复盘）上存在差异，影响最终效果。作者偏好Codex因其执行可靠且订阅制成本低。

### 解决方案：Wide Research架构与Codex结合
- **核心思路**: 将大规模任务分解为独立子问题（如每篇文章总结由轻量级LLM处理），避免单一LLM进行长输出，再汇总结果由主LLM润色。这解决了架构性约束。
- **在Codex上的实现**: 作者开发Repo，在Codex环境中复刻Wide Research，结合其高执行力和订阅优势。关键设计包括：
  - **流程自动化**: 设计AI自我迭代的工作流，通过测试集评估提示词，让AI自我反思瓶颈（如发现Codex CLI升级导致问题），减少人工干预。
  - **技术决策**: 子代理（Sub-Agent）形式选择Codex进程而非Python脚本，通过快速实现比较（仅10分钟开发时间）确定Codex方案在控制力、效果和成本上更优。
  - **网络访问优化**: 引入Tavily工具替代Codex自带搜索，解决网页访问不稳定、付费墙等问题，将任务速度提升2-3倍。
  - **Multi-Agent设计**: 避免生硬的角色分割（如PM/Engineer Agent），而是通过上下文隔离分配子问题，仅通过文件与主Agent通信，保持LLM全能性。

### 关键实践与启示
- **工程师角色转变**: 在AI时代，工程师应像高级管理者一样设计组织流程，聚焦于工作流设计和数据驱动迭代，而非微观管理。
- **开发效率提升**: 采用“实现优先”决策，直接比较方案而非过度分析；利用AI自动化减轻体力劳动。
- **开源与可复用性**: 所有提示词和代码已开源，鼓励用户通过教程体验Wide Research，强调架构和协作方式改进是解决AI“偷懒”的关键。

整体上，项目展示了如何通过分治策略、模型选择和工作流程优化，构建高效可靠的AI系统，并突出了AI时代工程师的核心竞争力在于系统设计和迭代能力。
