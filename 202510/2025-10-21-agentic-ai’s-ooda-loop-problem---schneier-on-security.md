# Agentic AI’s OODA Loop Problem - Schneier on Security
- URL: https://www.schneier.com/blog/archives/2025/10/agentic-ais-ooda-loop-problem.html
- Added At: 2025-10-21 13:53:08

## TL;DR
本文分析AI代理在对抗环境中的安全风险，指出OODA循环各阶段易受攻击，如数据投毒、提示注入等。根源在于模型无法保障语义完整性，且安全常为性能让步。需重新设计架构，将完整性内嵌。

## Summary
本文基于OODA循环框架，分析了智能AI代理在对抗性环境中面临的安全挑战，核心问题是AI系统在处理不可信输入时缺乏完整性保障。

### OODA循环框架及其应用
- OODA循环由Observe、Orient、Decide、Act四步组成，原用于描述战斗机飞行员的实时决策，现被应用于AI和机器人领域，代表AI代理在动态环境中迭代执行任务的过程。
- 传统OODA分析假设输入输出可信，但AI代理的实际运行环境充满敌意：网络连接的LLM可能查询对手控制的来源，工具调用API可能执行恶意代码，训练数据可能被投毒。这导致即使AI准确处理输入，输出仍可能完全腐败。

### AI代理的OODA循环安全风险
1. **Observe（观察）**：面临对抗性样本、提示注入、传感器欺骗等风险，观察层缺乏认证和完整性保障。
2. **Orient（定向）**：训练数据投毒、上下文操纵、语义后门等可提前影响模型的世界观，使其在部署后触发恶意行为。
3. **Decide（决策）**：微调攻击、奖励黑客、目标错位等可腐蚀决策逻辑，模型可能优先信任恶意来源。
4. **Act（行动）**：输出操纵、工具混淆、行动劫持等风险因协议（如MCP）扩大攻击面，工具调用隐含信任前序步骤。

风险被叠加放大：单个投毒数据可影响百万应用；训练与部署的时间差使漏洞无法审计；状态（如聊天记录）积累污染；多代理嵌套循环时，工具描述成为注入载体。

### 问题的根源
- AI需将现实压缩为模型可读形式，对手可攻击这种“地图”而非“领土”。模型处理符号而非意义，语义差距成为安全漏洞。
- 提示注入可能无解，因LLM无法标记令牌权限，任何解决方案（如分隔符、分层指令）都引入新攻击载体。安全需要边界，但LLM消解边界。
- 类似“信任的信任”攻击：污染状态生成污染输出，循环恶化。状态系统无法遗忘攻击，内存成为负担。
- AI安全存在三难抉择：快速、智能、安全不可兼得。优化前两者会牺牲验证，类似免疫系统误判自身组织的分子模拟。

### 未来方向
- 物理世界有约束（如雷达信号基于物理），但语义观察无物理验证，需语义完整性：验证解释、上下文和理解，而非仅数据。
- 计算机安全已解决可用性和保密性，当前需解决腐败下的完整性问题。完整性非附加功能，而是架构选择。
- 当前AI系统为“快速”和“智能”牺牲安全，代理越强大自主，缺乏完整性则越危险。必须重新设计架构，将完整性内置。

总之，AI的OODA循环因架构允许对手嵌入而脆弱，安全挑战本质是结构性的，需根本性变革而非修补。
