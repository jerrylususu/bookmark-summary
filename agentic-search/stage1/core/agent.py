#!/usr/bin/env python3
"""
æ ¸å¿ƒä»£ç†é€»è¾‘
ç»´æŠ¤å¯¹è¯å†å²ã€å·¥å…·é€‰æ‹©å’Œè°ƒç”¨å¾ªç¯ã€ç­”æ¡ˆç”Ÿæˆå’Œæ€»ç»“
"""

import json
import sys
import os
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ·»åŠ å·¥å…·ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'tools'))
# æ·»åŠ çˆ¶ç›®å½•åˆ° Python è·¯å¾„ä»¥å¯¼å…¥ embedding_pipeline
sys.path.append("/home/jerrylu/code/251028-bookmark-by-month/bookmark-summary")

try:
    from embedding_pipeline import TokenSplitter
    TOKENIZER_AVAILABLE = True
except ImportError as e:
    print(f"Tokenizer import warning: {e}")
    TOKENIZER_AVAILABLE = False

try:
    from keyword_search import keyword_search
    from vector_search import vector_search
    from text_reader import text_reader
except ImportError as e:
    print(f"Import error: {e}")

try:
    from llm_client import get_llm_client
    LLM_AVAILABLE = True
except ImportError as e:
    print(f"LLM client import warning: {e}")
    LLM_AVAILABLE = False

class AgenticAgent:
    def __init__(self):
        self.conversation_history = []
        self.tool_calls = []
        self.max_tool_calls = 10
        self.max_tokens = 60000
        self.current_tokens = 0

        # åˆå§‹åŒ–tokenizerï¼ˆä¸embedding_pipeline.pyä¿æŒä¸€è‡´ï¼‰
        if TOKENIZER_AVAILABLE:
            self.tokenizer = TokenSplitter(encoding_name="cl100k_base")
        else:
            self.tokenizer = None

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if not LLM_AVAILABLE:
            raise RuntimeError("LLMä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨ä»£ç†")

        try:
            self.llm_client = get_llm_client()
            self.use_llm = True
            print("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        }
        self.conversation_history.append(message)

    def estimate_tokens(self, text: str) -> int:
        """ä½¿ç”¨ä¸embedding_pipeline.pyä¸€è‡´çš„tokenizerä¼°ç®—tokenæ•°é‡"""
        if self.tokenizer:
            return self.tokenizer.count(text)
        else:
            # å›é€€åˆ°ç®€å•çš„å­—ç¬¦æ•°/4ä¼°ç®—
            return max(1, len(text) // 4)

    def check_token_limit(self, new_content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…è¿‡ token é™åˆ¶"""
        estimated_tokens = self.estimate_tokens(new_content)
        return (self.current_tokens + estimated_tokens) < self.max_tokens

    def choose_tool(self, query: str, previous_results: List[Dict]) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®å½“å‰æƒ…å†µé€‰æ‹©å·¥å…·

        Returns:
            tool_call dict æˆ– None
        """
        # å¦‚æœå¯ç”¨LLMï¼Œä½¿ç”¨LLMè¿›è¡Œå·¥å…·é€‰æ‹©
        if self.use_llm and self.llm_client:
            available_tools = ["keyword_search", "vector_search", "text_reader"]
            try:
                llm_decision = self.llm_client.choose_tool(
                    query, available_tools, previous_results, self.conversation_history
                )
                if llm_decision:
                    print(f"ğŸ¤– LLMé€‰æ‹©å·¥å…·: {llm_decision['tool']} (native tool calling)")
                    return {
                        "tool": llm_decision["tool"],
                        "params": llm_decision["params"]
                    }
                else:
                    print("ğŸ¤– LLMè®¤ä¸ºæœç´¢å·²å®Œæˆ")
                    return None
            except Exception as e:
                print(f"âŒ LLMå·¥å…·é€‰æ‹©å¤±è´¥: {e}")
                raise ValueError(f"LLMå·¥å…·é€‰æ‹©å¤±è´¥: {e}")

        # è¿™ä¸ªæ–¹æ³•åœ¨LLMæ¨¡å¼ä¸‹ä¸åº”è¯¥è¢«è°ƒç”¨
        raise RuntimeError("åœ¨LLMæ¨¡å¼ä¸‹ï¼Œå·¥å…·é€‰æ‹©åº”è¯¥å®Œå…¨ç”±LLMå¤„ç†ï¼Œä¸åº”åˆ°è¾¾è¿™é‡Œçš„å›é€€é€»è¾‘")

    def execute_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        tool_name = tool_call["tool"]
        params = tool_call["params"]

        try:
            if tool_name == "keyword_search":
                result = keyword_search(**params)
            elif tool_name == "vector_search":
                result = vector_search(**params)
            elif tool_name == "text_reader":
                result = text_reader(**params)
            else:
                result = {
                    "success": False,
                    "error": f"Unknown tool: {tool_name}"
                }

            # è®°å½•å·¥å…·è°ƒç”¨
            call_record = {
                "tool": tool_name,
                "params": params,
                "result": result,
                "timestamp": datetime.now().isoformat()
            }
            self.tool_calls.append(call_record)

            # åœ¨ç»“æœä¸­æ·»åŠ å·¥å…·ä½¿ç”¨ä¿¡æ¯ï¼Œä¾›LLMå†³ç­–å‚è€ƒ
            result["tool_used"] = tool_name

            return result

        except Exception as e:
            error_result = {
                "success": False,
                "error": str(e),
                "tool": tool_name,
                "params": params,
                "tool_used": tool_name  # æ·»åŠ å·¥å…·ä½¿ç”¨ä¿¡æ¯
            }
            self.tool_calls.append({
                "tool": tool_name,
                "params": params,
                "result": error_result,
                "timestamp": datetime.now().isoformat()
            })
            return error_result

    def should_continue_search(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­æœç´¢"""
        if len(self.tool_calls) >= self.max_tool_calls:
            return False

        # å¦‚æœæœ€è¿‘å‡ æ¬¡æœç´¢éƒ½æ²¡æœ‰ç»“æœï¼Œåœæ­¢æœç´¢
        recent_failures = sum(1 for call in self.tool_calls[-3:]
                            if not call["result"].get("success") or
                               not call["result"].get("results"))
        if recent_failures >= 2:
            return False

        return True

    def generate_answer(self, query: str) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        # æ”¶é›†æ‰€æœ‰æœ‰ç”¨ä¿¡æ¯
        all_results = []
        search_results_with_tools = []

        for call in self.tool_calls:
            result = call["result"]
            if result.get("success") and result.get("results"):
                all_results.extend(result["results"])
                # ä¸ºLLMå‡†å¤‡åŒ…å«å·¥å…·ä¿¡æ¯çš„ç»“æœ
                search_results_with_tools.append({
                    "tool_name": call["tool"],
                    "success": result.get("success", False),
                    "results": result.get("results", [])
                })

        # å¦‚æœå¯ç”¨LLMä¸”ç»“æœä¸ä¸ºç©ºï¼Œä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
        if self.use_llm and self.llm_client and search_results_with_tools:
            try:
                print("ğŸ¤– ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ...")
                llm_answer = self.llm_client.generate_answer(query, search_results_with_tools)
                print("âœ… LLMç­”æ¡ˆç”Ÿæˆå®Œæˆ")

                # ç”Ÿæˆæ¥æºåˆ—è¡¨
                sources = []
                for call in self.tool_calls:
                    result = call["result"]
                    if result.get("success") and result.get("results"):
                        for item in result["results"]:
                            if item.get('file_path'):
                                sources.append(item['file_path'])
                            elif item.get('document_id'):
                                sources.append(item['document_id'])

                sources = list(set(sources))  # å»é‡

                return {
                    "success": True,
                    "answer": llm_answer,
                    "sources": sources,
                    "tool_calls_count": len(self.tool_calls),
                    "confidence": "high" if len(all_results) > 3 else "medium"
                }
            except Exception as e:
                print(f"âŒ LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
                raise ValueError(f"LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")

        # å¦‚æœæ²¡æœ‰æœç´¢ç»“æœï¼Œè¿”å›ç©ºç­”æ¡ˆ
        if not all_results:
            return {
                "success": True,
                "answer": f"å¾ˆæŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ã€Œ{query}ã€ç›¸å…³çš„ä¿¡æ¯ã€‚",
                "sources": [],
                "tool_calls_count": len(self.tool_calls),
                "confidence": "low"
            }

        # ç”Ÿæˆç­”æ¡ˆ
        answer_parts = [f"æ ¹æ®æœç´¢ç»“æœï¼Œå…³äºã€Œ{query}ã€çš„å›ç­”å¦‚ä¸‹ï¼š\n"]

        # æŒ‰å·¥å…·ç±»å‹åˆ†ç»„æ•´ç†ç»“æœ
        keyword_results = []
        vector_results = []
        text_results = []

        for call in self.tool_calls:
            result = call["result"]
            if result.get("success") and result.get("results"):
                if call["tool"] == "keyword_search":
                    keyword_results.extend(result["results"])
                elif call["tool"] == "vector_search":
                    vector_results.extend(result["results"])
                elif call["tool"] == "text_reader":
                    text_results.append(result)

        # æ·»åŠ å‘é‡æœç´¢ç»“æœï¼ˆè¯­ä¹‰ç›¸å…³ï¼‰
        if vector_results:
            answer_parts.append("## è¯­ä¹‰ç›¸å…³å†…å®¹ï¼š")
            for i, result in enumerate(vector_results[:3]):
                answer_parts.append(f"{i+1}. **{result.get('title', 'æœªçŸ¥æ ‡é¢˜')}** ({result.get('month', 'æœªçŸ¥æ—¶é—´')})")
                if result.get('heading'):
                    answer_parts.append(f"   ç« èŠ‚: {result['heading']}")
                answer_parts.append(f"   ç›¸ä¼¼åº¦: {result.get('similarity', 0):.3f}")
                answer_parts.append(f"   æ‘˜è¦: {result.get('content', '')[:200]}...")
                answer_parts.append("")

        # æ·»åŠ å…³é”®è¯æœç´¢ç»“æœ
        if keyword_results:
            answer_parts.append("## å…³é”®è¯åŒ¹é…å†…å®¹ï¼š")
            for i, result in enumerate(keyword_results[:3]):
                answer_parts.append(f"{i+1}. **æ–‡ä»¶**: {result.get('file_path', '')}")
                answer_parts.append(f"   ä½ç½®: ç¬¬{result.get('line_number', 0)}è¡Œ")
                answer_parts.append(f"   å†…å®¹: {result.get('content', '')[:150]}...")
                answer_parts.append("")

        # æ·»åŠ æ–‡æœ¬è¯»å–ç»“æœ
        if text_results:
            answer_parts.append("## è¯¦ç»†å†…å®¹ï¼š")
            for result in text_results[:2]:
                answer_parts.append(f"**æ–‡ä»¶**: {result.get('file_path', '')}")
                answer_parts.append(f"**è¡ŒèŒƒå›´**: {result.get('start_line', 0)}-{result.get('end_line', 0)}")
                content = result.get('content', '')[:500]
                answer_parts.append(f"**å†…å®¹é¢„è§ˆ**:\n```\n{content}\n```")
                answer_parts.append("")

        # ç”Ÿæˆæ¥æºåˆ—è¡¨
        sources = []
        for call in self.tool_calls:
            result = call["result"]
            if result.get("success") and result.get("results"):
                for item in result["results"]:
                    if item.get('file_path'):
                        sources.append(item['file_path'])
                    elif item.get('document_id'):
                        sources.append(item['document_id'])

        sources = list(set(sources))  # å»é‡

        final_answer = {
            "success": True,
            "answer": "\n".join(answer_parts),
            "sources": sources,
            "tool_calls_count": len(self.tool_calls),
            "confidence": "high" if len(all_results) > 3 else "medium"
        }

        return final_answer

    def process_query(self, query: str) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ä¸»è¦æµç¨‹"""
        self.add_message("user", query)

        print(f"\n=== å¼€å§‹å¤„ç†æŸ¥è¯¢: {query} ===")

        while self.should_continue_search():
            # é€‰æ‹©å·¥å…·
            tool_call = self.choose_tool(query, [call["result"] for call in self.tool_calls])
            if not tool_call:
                print("æ²¡æœ‰åˆé€‚çš„å·¥å…·å¯ä»¥ç»§ç»­æœç´¢")
                break

            print(f"\n--- è°ƒç”¨å·¥å…·: {tool_call['tool']} ---")
            print(f"å‚æ•°: {json.dumps(tool_call['params'], ensure_ascii=False, indent=2)}")

            # æ‰§è¡Œå·¥å…·
            result = self.execute_tool(tool_call)

            print(f"ç»“æœ: {'æˆåŠŸ' if result.get('success') else 'å¤±è´¥'}")
            if result.get("results"):
                print(f"æ‰¾åˆ° {len(result['results'])} æ¡ç»“æœ")
                # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœçš„æ‘˜è¦ï¼Œä¾¿äºç†è§£LLMè·å¾—äº†ä»€ä¹ˆä¿¡æ¯
                for i, item in enumerate(result["results"][:2], 1):
                    if tool_call['tool'] == "vector_search":
                        title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')[:50]
                        content = item.get('content', '')[:100]
                        print(f"  ç»“æœ{i}: {title} - {content}...")
                    elif tool_call['tool'] == "keyword_search":
                        file_path = item.get('file_path', '')
                        line_num = item.get('line_number', 0)
                        content = item.get('content', '')[:80]
                        print(f"  ç»“æœ{i}: {file_path}:{line_num} - {content}...")
                    elif tool_call['tool'] == "text_reader":
                        file_path = item.get('file_path', '')
                        content = item.get('content', '')[:100]
                        print(f"  ç»“æœ{i}: {file_path} - {content}...")
            elif result.get("error"):
                print(f"é”™è¯¯: {result['error']}")

            # å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
            tool_result_summary = f"è°ƒç”¨äº†å·¥å…· {tool_call['tool']}ï¼Œå‚æ•°ï¼š{json.dumps(tool_call['params'], ensure_ascii=False)}\nç»“æœï¼š"
            if result.get("success"):
                if result.get("results"):
                    tool_result_summary += f"æˆåŠŸæ‰¾åˆ° {len(result['results'])} æ¡ç»“æœã€‚\nå‰å‡ ä¸ªç»“æœæ‘˜è¦ï¼š\n"
                    for i, item in enumerate(result["results"][:3], 1):
                        if tool_call['tool'] == "vector_search":
                            tool_result_summary += f"{i}. {item.get('title', 'æœªçŸ¥æ ‡é¢˜')} - {item.get('content', '')[:150]}...\n"
                        elif tool_call['tool'] == "keyword_search":
                            tool_result_summary += f"{i}. {item.get('file_path', '')}:{item.get('line_number', 0)} - {item.get('content', '')[:100]}...\n"
                        elif tool_call['tool'] == "text_reader":
                            tool_result_summary += f"{i}. {item.get('file_path', '')} å†…å®¹ç‰‡æ®µ: {item.get('content', '')[:150]}...\n"
                else:
                    tool_result_summary += "æˆåŠŸä½†æ— åŒ¹é…ç»“æœã€‚\n"
            else:
                tool_result_summary += f"å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}\n"

            self.add_message("assistant", tool_result_summary)

        # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        print(f"\n=== ç”Ÿæˆç­”æ¡ˆ ===")
        final_answer = self.generate_answer(query)

        self.add_message("assistant", final_answer.get("answer", ""))

        return final_answer

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    agent = AgenticAgent()
    result = agent.process_query("LLM embedding")
    print("\n" + "="*50)
    print("æœ€ç»ˆç­”æ¡ˆ:")
    print(result.get("answer", ""))
    print(f"\næ¥æº: {result.get('sources', [])}")
    print(f"å·¥å…·è°ƒç”¨æ¬¡æ•°: {result.get('tool_calls_count', 0)}")