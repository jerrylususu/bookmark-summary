# 图解神经网络和强化学习：400 行 C 代码训练一个井字棋高手（2025）
- URL: https://arthurchiao.github.io/blog/reinforcement-learning-400-lines-of-c-code-zh/
- Added At: 2025-03-21 14:26:38

## TL;DR


本文介绍了通过400行C代码实现的井字棋强化学习项目。使用含100节点隐藏层的神经网络，经200万局对弈训练后，AI胜率达84%，平局15%，几乎不败。代码仅依赖标准库，训练速度极快（150万局数秒内完成），验证神经网络本质为数学运算组合，非复杂技术黑箱。

## Summary


本文介绍了通过400行C代码实现的井字棋强化学习项目。项目使用一个简单的神经网络（1个隐藏层，节点数100），通过与随机对手对弈200万局进行训练，最终实现高胜率的人机对战。代码无外部依赖，仅使用标准库函数。

### 核心组件
1. **棋盘状态**：使用3x3字符数组表示棋盘，`X`为玩家，`O`为AI，`.`为空。状态结构包含当前玩家标记。
2. **神经网络结构**：
   - 输入层：18（将棋盘每格状态编码为2bit，共9格）。
   - 隐藏层：100节点，使用ReLU激活。
   - 输出层：9节点（对应棋盘位置），用softmax输出概率。
   - 总参数量：2809（18×100 + 100×9 + 100 + 9）。
3. **输入编码**：
   - 使用`00`代表空格，`10`代表`X`，`01`代表`O`，将棋盘状态转换为浮点向量。

### 训练过程
1. **训练循环**：
   - AI与随机玩家对弈200万局，每局后根据胜负反向传播调整权重。
   - 训练后模型保存为`ttt_nn.bin`，大小约11KB。
2. **奖励策略**：
   - 赢得+1.0奖励，平局+0.3，输-2.0。
   - 局部奖励随游戏进程增加（后期落子奖励权重更高）。
3. **反向传播**：
   - 回放每步决策，以实际落子构建目标概率（目标位置100%，其余0%）。
   - 通过`(输出[i] - 目标[i])`计算误差，更新权重（学习率固定）。

### 代码模块
1. **`common.h`**：
   - 包含神经网络定义、状态转移函数`board_to_inputs`、前向传播`forward_pass`。
   - 激活函数实现：`relu={()=>max(0,x)`，`softmax`通过指数归一化处理。
2. **`train.c`**：
   - `init_neural_network`随机初始化权重（-0.5~0.5）。
   - `learn_from_game`基于对局结果计算奖励并调用`backprop`更新网络。
3. **`play.c`**：
   - 加载训练后的模型，实现人机交互界面，按概率选择最优空位落子。

### 结果
- 训练200万局后，AI胜率超过84%，平局约15%，几乎无败局。
- 训练速度极快，150万局可在数秒内完成（具体取决于机器性能）。
- 修改隐藏层参数（如降为25节点）仍可保持较高竞争力。

### 延伸思考
- 现实中可用蒙特卡洛树搜索（MCTS）优化策略。
- 激活函数选择（如Sigmoid/Tanh）、权重初始化方法（如He/Xavier）可进一步实验。
- 当前简单编码方式启发了用更高效的特征表示替代传统独热编码（One-Hot）。
- 代码结构展示神经网络本质是数学运算组合，非神秘复杂的技术黑箱。
