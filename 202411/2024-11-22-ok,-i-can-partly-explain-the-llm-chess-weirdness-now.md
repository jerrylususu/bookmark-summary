# OK, I can partly explain the LLM chess weirdness now
- URL: https://dynomight.net/more-chess/
- Added At: 2024-11-22 13:36:45

## TL;DR
文章探讨了大型语言模型（LLMs）在棋类游戏中的表现，特别是`gpt-3.5-turbo-instruct`的优异表现。通过实验和理论分析，发现提示调整和示例提供能显著提升模型表现，而OpenAI作弊的可能性极低。最终理论指出，基础模型在棋类数据上训练得更好，但聊天模型在指令调优后表现下降。未来工作将探索更多提示和示例组合，以进一步提升模型表现。

## Summary
1. **问题背景**：
   - 所有大型语言模型（LLMs）在棋类游戏中表现不佳，除了`gpt-3.5-turbo-instruct`，它在某些情况下能达到高级业余水平。
   - `gpt-3.5-turbo-instruct`虽然较旧且较小，但表现异常出色。

2. **初步解释**：
   - **理论1**：足够大的基础模型擅长下棋，但在指令调优为聊天模型后，这种能力消失。
   - **理论2**：`gpt-3.5-turbo-instruct`可能接受了更多棋类数据的训练。
   - **理论3**：某些LLM架构具有魔力。
   - **理论4**：不同类型的数据之间存在竞争，因此LLM要下好棋，需要大量棋类游戏数据。

3. **互联网其他理论**：
   - **理论5**：OpenAI在作弊。
   - **理论6**：LLMs实际上不能下棋。
   - **理论7**：大型基础模型擅长下棋，但在指令调优为聊天模型后，这种能力消失。

4. **实验结果**：
   - 最新聊天模型在经过极端提示调整后，可以下得相当好。
   - `gpt-3.5-turbo-instruct`在棋类游戏中表现出色，而其他模型表现糟糕。

5. **OpenAI作弊的可能性**：
   - 作者认为OpenAI作弊的可能性极低，理由包括：
     - OpenAI员工否认作弊。
     - `gpt-3.5-turbo-instruct`对不同棋局序列反应不同。
     - 即使作弊，也不应仅限于业余水平。
     - 提示方式改变会影响棋局表现。
     - 后续模型默认表现更差。
     - 后续模型在正确提示下也能表现良好。

6. **LLMs下棋的能力**：
   - LLMs确实能下棋，且能理解棋局规则。
   - `gpt-3.5-turbo-instruct`很少提出非法走法，表明其理解棋局。
   - 提供了十个实际棋局链接，证明LLMs在全新棋局中也能表现良好。

7. **实验方法**：
   - 使用不同模型（`gpt-3.5-turbo-instruct`、`gpt-4o-mini`、`gpt-4o`）与Stockfish对战，记录每步棋的得分。
   - 结果显示`gpt-3.5-turbo-instruct`表现优异，而其他模型表现不佳。

8. **提示调整实验**：
   - **重复系统提示**：对`gpt-4o-mini`影响不大，对`gpt-4o`可能有影响。
   - **提供示例**：三个小示例显著提升了模型表现。
   - **微调**：微调帮助模型提升表现。
   - **结合示例和微调**：示例使微调变得多余。
   - **提供合法走法**：列出合法走法反而使模型表现更差。

9. **新想法**：
   - **重复棋局**：要求模型重复整个棋局后再给出新走法，显著提升模型表现。
   - **结合重复和微调**：重复和微调结合可能帮助模型进一步提升。
   - **结合重复和示例**：重复和示例结合显著提升模型表现。

10. **总结**：
    - **有效方法**：重复棋局、提供示例、微调（不结合示例）。
    - **不确定方法**：元数据、重复系统提示、微调（结合示例）。
    - **无效方法**：提供合法走法。

11. **最终理论**：
    - **OpenAI基础模型**：OpenAI的基础模型在棋类数据上训练得更好，因此表现优异。
    - **聊天模型问题**：聊天模型在指令调优后表现下降，可能是由于聊天模板的影响。

12. **未来工作**：
    - 探索更多提示和示例组合，进一步提升模型表现。
    - 寻找最优提示组合的挑战巨大，需要大量时间和资源。
    - 不同模型可能需要不同的提示策略。

13. **致谢和参考**：
    - 感谢Automator和Daniel Gross的支持。
    - 参考了多个关于LLMs和棋类的研究工作。
