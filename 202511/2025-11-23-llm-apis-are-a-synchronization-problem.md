# LLM APIs are a Synchronization Problem
- URL: https://lucumr.pocoo.org/2025/11/22/llm-apis/
- Added At: 2025-11-23 10:46:21
- Tags: #read #llm #distributed

## TL;DR
文章指出当前LLM API设计存在底层状态与消息抽象不匹配的问题，导致同步困难和效率低下。建议借鉴本地优先软件的状态同步理念，将对话历史作为可增量同步的日志，而非全量传输，并倡导未来API转向明确状态管理的设计标准。

## Summary
文章《LLM API 是一个同步问题》主要探讨了当前大语言模型提供商 API 的设计缺陷，并提出了以状态同步为核心的新思路。以下是结构化总结：

## 核心问题：API 抽象与底层机制不匹配
- 当前主流 LLM API（如 OpenAI 或 Anthropic 的完成式 API）基于消息传递抽象，但这与模型底层的运行机制存在根本差异。
- 模型内部处理的是 token 序列，涉及 GPU 上的注意力机制和 KV 缓存等状态，而 API 通过 JSON 消息隐藏了细节（如提示模板、工具定义），导致状态同步问题。

## 当前 API 的局限性
- **完成式 API**：每次请求需重复发送整个对话历史，导致数据量呈二次增长，成本高且效率低。服务器端缓存机制不透明，用户无法控制状态。
- **Responses API**：尝试在服务器端维护状态，但同步能力有限，容易出现状态分歧、网络分区无法恢复等问题，且隐藏状态过多，用户难以调试。

## 状态同步是关键挑战
- 提供商在后台注入隐藏上下文（如系统提示、工具输出），不同提供商处理方式不兼容，缺乏统一标准。
- 中间层（如 OpenRouter 或 Vercel AI SDK）试图统一接口，但无法解决底层状态同步的复杂性。

## 借鉴本地优先运动
- 本地优先软件领域已成熟处理分布式状态同步（如 CRDT、离线协作），其分离规范状态、派生状态和传输机制的理念可应用于 LLM API。
- 建议将 KV 缓存视为可检查点的派生状态，对话历史作为增量同步的日志，而非全量重传。

## 未来 API 设计方向
- 应摒弃当前以消息为中心的抽象，转向基于状态同步的 API 标准，明确隐藏状态、同步边界和故障恢复机制。
- 避免过早固化现有有缺陷的抽象，需从模型实际行为出发设计，支持状态重放和容错。

文章最终强调，LLM API 的演变需正视同步问题，而非停留在表面消息格式的统一。
