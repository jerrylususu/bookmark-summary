# Why it takes months to tell if new AI models are good
- URL: https://www.seangoedecke.com/are-new-models-good/
- Added At: 2025-11-23 10:36:54
- Tags: #read #llm #eval
- [Link To Text](2025-11-23-why-it-takes-months-to-tell-if-new-ai-models-are-good_raw.md)

## TL;DR
评估AI模型质量面临三大难题：现有基准易被操纵且脱离实际，主观判断不可靠，真实场景测试又耗时费力。当模型智能超越人类后，进步更难被感知，导致AI是否停滞的争议无解。

## Summary
评估新AI模型的质量通常需要数月时间，原因如下：

### 评估基准的局限性
- **评估基准不可靠**：尽管评估数据集是衡量模型性能的标准方法，但它们往往高估前沿模型的实际能力。
- **难以设计真实场景测试**：现实问题涉及大量上下文，而评估通常无法覆盖这种复杂性。例如，开源代码评估仅针对特定语言（如Python），无法代表所有编程工作。
- **评估被公司操纵**：AI公司可能通过针对性训练优化模型在评估中的表现，导致评估结果无法反映真实能力，更多是营销工具。

### 主观判断的不足
- **“感觉检查”不可靠**：用户通过互动或特定问题（如字谜或艺术提示）来主观判断模型质量，但这些方法无法测试实际工作能力，且容易受偏见影响。
- **纯感觉判断易误导**：依赖“模型气味”等无形感受可能产生误判，例如模型对话风格可能影响对其智能的感知。

### 实际测试的耗时性
- **真实使用需投入时间**：要准确评估模型，必须在实际工作场景中测试，但这需要用户亲自参与并对比结果，过程耗时且有风险（如模型失败会导致时间浪费）。
- **决策困难**：面对新模型（如Gemini 3 Pro或GPT-5.1-Codex），用户需权衡测试成本，导致评估延迟。

### AI进步是否停滞的争议
- **缺乏可靠评估方法**：模型发布常引发AI进步是否停滞的争论，但核心问题在于无法客观衡量模型智能。
- **模型超越人类后难以感知进步**：当模型智能超过人类时，用户可能无法察觉其改进，就像棋艺差的玩家无法区分不同水平的象棋引擎。模型从低于人类到持平阶段进步明显，但超越后进步感会 plateau（趋于平缓）。

### 总结
- 模型发布初期质量难判，连开发公司也仅能推测。
- 评估基准受营销影响，主观测试不实用，而实际测试又费时。
- 这些因素使得AI进步是否停滞成为无解之争，且模型越智能，人类越难感知其提升。
