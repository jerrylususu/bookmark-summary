# What's so hard about continuous learning?
- URL: https://www.seangoedecke.com/continuous-learning/
- Added At: 2026-02-24 11:33:18
- Tags: #read #llm

## TL;DR
连续学习指模型部署后持续更新权重，但面临技术难题、微调无效、安全风险和可移植性差等障碍，核心难点在于自动避免性能退化，目前仍需人工干预，尚未成熟。

## Summary
连续学习（continuous learning）指模型在部署后能持续更新权重、提升能力，但目前尚未实现。文章指出，连续学习的技术实现并不困难——只需在运行时通过训练管道更新权重即可，这与模型后训练过程类似。然而，真正的挑战在于如何确保模型在持续学习中变得更好而非更差，因为模型训练对数据和计算的响应并非线性，且存在随机性，需要人工监督和精细调控。

此外，连续学习面临多重障碍：
1. **技术难题**：模型训练易陷入局部最优或性能下降，且对随机种子敏感，难以保证稳定提升。
2. **微调无效**：针对特定代码库的微调（fine-tuning）效果不佳，无法让模型真正理解代码库逻辑，这削弱了连续学习的实用性。
3. **安全风险**：连续学习可能引入权重注入攻击，导致模型被恶意数据毒化，风险远高于当前的提示注入攻击。
4. **可移植性差**：模型学到的知识编码在权重中，升级到新版本时难以迁移，用户可能面临数据丢失或无法升级的困境。

总之，连续学习的核心难点不在于“连续”而在于“自动”——目前仍需人工干预来避免模型性能退化。同时，安全性和可移植性问题也限制了其产品化。尽管AI实验室可能在内部进行持续训练，但面向用户的连续学习功能尚未成熟，且微调实践的失败表明，实现真正的持续学习仍需解决基础学习问题。
